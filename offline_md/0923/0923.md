package com.example;

import com.ververica.cdc.connectors.mysql.source.MySqlSource;
import com.ververica.cdc.debezium.StringDebeziumDeserializationSchema;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class App {
public static void main(String[] args) throws Exception {
// Flink 环境
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setParallelism(1);

        EnvironmentSettings settings = EnvironmentSettings.newInstance()
                .inStreamingMode()
                .build();
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);

        // CDC Source 读取 MySQL 表
        MySqlSource<String> mySqlSource = MySqlSource.<String>builder()
                .hostname("192.168.200.32")
                .port(3306)
                .username("root")
                .password("root")
                .databaseList("mysql_test")
                .tableList("mysql_test.user")
                .deserializer(new StringDebeziumDeserializationSchema())
                .build();

        DataStreamSource<String> source = env.fromSource(mySqlSource,
                WatermarkStrategy.noWatermarks(),
                "MySQLSource");

        // 注册 MySQL CDC 数据为临时表
        tEnv.executeSql("CREATE TABLE ods_user_source (" +
                " id INT," +
                " name STRING," +
                " age INT," +
                " dt STRING" +
                ") WITH (" +
                " 'connector' = 'values'," +
                " 'bounded' = 'false')");

        // Hive Sink
        tEnv.executeSql("CREATE TABLE work.ods_user (" +
                " id INT," +
                " name STRING," +
                " age INT" +
                ") PARTITIONED BY (dt STRING) " +
                " STORED AS PARQUET " +
                " TBLPROPERTIES (" +
                " 'partition.time-extractor.timestamp-pattern'='$dt'," +
                " 'sink.partition-commit.policy.kind'='metastore,success-file')");

        // 将 CDC 数据写入 Hive 表
        tEnv.executeSql("INSERT INTO work.ods_user SELECT id, name, age, dt FROM ods_user_source");

        env.execute("Flink CDC MySQL -> Hive Demo");
    }
}
